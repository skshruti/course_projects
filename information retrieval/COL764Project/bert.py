# -*- coding: utf-8 -*-
"""info.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/151Ti6wm27SQEIy-MTM3yh1difM-4P1Si
"""

import json

from rake_nltk import Rake

from rank_bm25 import BM25Okapi
rake_nltk_var = Rake()

corpusmap = json.load(open('./sample_data/corpusmaphalfmn.json'))
corpus = json.load(open('./sample_data/corpussmallhalfmn.json'))

def getQueries(filename='./sample_data/train.json'):
    queries = {}
    his_query = {}
    traindata = json.load(open(filename))
    for i, conversation in enumerate(traindata):
        if(i+1 < len(traindata) and traindata[i+1]['Question_no']==1):
          #qid = conversation["QuAC_dialog_id"]+'@'+str(conversation["Question_no"])
          qid = i
          history = " ".join(conversation["History"])
          rake_nltk_var.extract_keywords_from_text(history)
          keyword_extracted = rake_nltk_var.get_ranked_phrases()
          queries[qid] = keyword_extracted[0] + ' ' +conversation["Question"]
          his_query[qid] = (conversation["History"],conversation["Question"])
        if(len(queries)==100): break
    return queries, his_query
queries, his_query = getQueries()

def getIdealQueries(filename='./sample_data/train.json'):
    queries = {}
    traindata = json.load(open(filename))
    for i, conversation in enumerate(traindata):
        if(i+1 < len(traindata) and traindata[i+1]['Question_no']==1):
        # qid = conversation["QuAC_dialog_id"]+'@'+str(conversation["Question_no"])
          qid = i
          queries[qid] = conversation["Rewrite"]
        #if(len(queries)==2): break
    return queries

def rankBM25(corpus, queries, k):
    corpuslist = list(corpus.values())
    corpuslist = [doc.split(" ") for doc in corpuslist]
    bm25 = BM25Okapi(corpuslist)
    rankedDocs = {}
    i = 0
    l = len(queries)
    for qid in queries:
        print(qid, i, l)
        i+=1
        query = (queries[qid]).split(' ')
        doc_scores = bm25.get_scores(query)
        indices = np.argpartition(doc_scores, -1*k)[-1*k:]
        indices = indices[np.argsort(doc_scores[indices])]
        indices = [indices[len(indices)-1-k] for k in range(len(indices))]
        rankedDocs[qid]=[corpusmap[str(index)] for index in indices]
        # rankedDocs[qid] = indices
    return rankedDocs

from transformers import BertTokenizer, BertModel
from scipy.spatial.distance import cosine
from sentence_transformers import SentenceTransformer, util
from sklearn.datasets import load_iris
import pandas as pd
import numpy as np
import nltk
import torch
import pandas as pd
import numpy as np
import torch
from transformers import BertForQuestionAnswering
from transformers import BertTokenizer

#In this I am assuming to get the top passages for
#a query need q(k) in a list top_paragraphs (top 200 maybe)
#The history of q that is q(k-w) to q(k-1) in
#a list query_history

#window = 4
w = 4

query_historylist = list(his_query.values())
query_ids = list(his_query.keys())
#top_paragraphs = list(corpus.values())
new_queries = []

for (history,query) in query_historylist:
    q_reformed = ""
    count = 0
    for i in reversed(range(len(history))) :
        count += 1
        q_reformed = history[i]+"[SEP]"+q_reformed
        if(count == w):
            if(i==0): break
            else:
                q_reformed = history[0]+"[SEP]"+q_reformed
                break
        
    q_reformed = "[CLS]" + q_reformed + query + "[SEP]"
    new_queries.append(q_reformed)


# model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

model = SentenceTransformer('bert-base-nli-mean-tokens')
# model = SentenceTransformer('bert-large-uncased-whole-word-masking-finetuned-squad')

query_embeddings = []
for q in new_queries:
    query_embed = model.encode(q)
    query_embeddings.append(query_embed)
    
    
Similarity = {}
for i in range(len(query_embeddings)):
    q = query_embeddings[i]
    passage_embeddings = []
    top_paragraphs = list(querytopdocs[query_ids[i]].values())
    for p in top_paragraphs:
        pstrings = p.split(".")
        passage_embeddings.append(pstrings)
    Sim = {}
    for j in range(len(passage_embeddings)):
       p = model.encode(passage_embeddings[j])
      #  simscore = 0
      #  for p_emb in p:
      #      simscore += util.dot_score(q, p_emb)
      #  simscore = simscore/len(passage_embeddings)
       
       simscore = util.dot_score(q, p)
       l = [val for val in simscore]
       m = np.array([float(v) for v in l[0]])
       Sim[j] = np.sum(m)/len(m)
       print(Sim[j])
    Sim = sorted(Sim, key = lambda i: Sim[i], reverse=True)[:20]
    Similarity[i] = Sim
Similarity

qid_doc_rank={}
for q in Similarity:  
  temp = Similarity[q]
  qid_doc_rank[q]=[corpusmap[str(index)] for index in temp]

queries = getIdealQueries()

topDocs_queries = rankBM25(corpus, queries, 20)

topDocs_queries

qid_doc_rank

def evaluate(ranked_queries, topDocs_queries, queries, N):   
    mrr = 0    
    meanap = 0     
    fscores = 0 
    recall = 0 
    precision = 0 
    for query in queries:  
        avgp = 0
        # results = []
        mrrq = -1
        topDocs = topDocs_queries[query]
        ranked = ranked_queries[query]
        tp = len(set(topDocs).intersection(set(ranked)))
        fn = len(topDocs) - tp
        fp = len(ranked) - tp
        fscores += tp/(tp+(fp+fn)/2)
        recall += tp/(tp+fn)
        precision += tp/(tp+fp)
        for i in range(len(ranked)):
            if(ranked[i] in topDocs and mrrq == -1): mrrq = 1/(i+1)
        meanap += avgp/100
        if mrrq!=-1: 
            mrr += mrrq
    print('MRR: ', mrr/len(queries), 'Avg F1 score: ', fscores/len(queries),'Avg Precision: ', precision/len(queries),'Avg Recall: ', recall/len(queries))

evaluate(qid_doc_rank, topDocs_queries, queries, 50000)

def getQueriestemp(filename='./sample_data/train.json'):
    queries = {}
    his_query = {}
    traindata = json.load(open(filename))
    for i, conversation in enumerate(traindata):
        if(i+1 < len(traindata) and traindata[i+1]['Question_no']==1):
          #qid = conversation["QuAC_dialog_id"]+'@'+str(conversation["Question_no"])
          qid = i
          history = " ".join(conversation["History"][1:])
          rake_nltk_var.extract_keywords_from_text(history)
          keyword_extracted = rake_nltk_var.get_ranked_phrases()
          queries[qid] = conversation["History"][0] +' '+ keyword_extracted[0] + ' ' +conversation["Question"]
          his_query[qid] = (conversation["History"],conversation["Question"])
        if(len(queries)==100): break
    return queries, his_query

queries, his_query = getQueriestemp()

top200docs = rankBM25(corpus, queries, 200)

top200docs

querytopdocs = {}
for q in top200docs:
  querytopdocs[q] = {}
  for doc in top200docs[q]:
    querytopdocs[q][doc] = corpus[str(doc)]

len(querytopdocs), len(corpus)

